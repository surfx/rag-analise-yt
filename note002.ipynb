{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ARQUIVOS = r\"data\"\n",
    "LANG = \"por\" # por eng\n",
    "persist_directory = \"chroma/chroma_db\"  # Diretório onde o banco de dados será salvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode, EasyOcrOptions, TesseractOcrOptions, OcrMacOptions\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "# Define pipeline options for PDF processing\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    do_table_structure=True,  # Enable table structure detection\n",
    "    do_ocr=True,  # Enable OCR\n",
    "    # full page ocr and language selection\n",
    "    #ocr_options=EasyOcrOptions(force_full_page_ocr=True, lang=[\"en\"]),  # Use EasyOCR for OCR\n",
    "    ocr_options=TesseractOcrOptions(force_full_page_ocr=True, lang=[LANG]),  # Uncomment to use Tesseract for OCR\n",
    "    #ocr_options = OcrMacOptions(force_full_page_ocr=True, lang=['en-US']),\n",
    "    table_structure_options=dict(\n",
    "        do_cell_matching=False,  # Use text cells predicted from table structure model\n",
    "        mode=TableFormerMode.ACCURATE  # Use more accurate TableFormer model\n",
    "    ),\n",
    "    generate_page_images=True,  # Enable page image generation\n",
    "    generate_picture_images=True,  # Enable picture image generation\n",
    "    images_scale=IMAGE_RESOLUTION_SCALE, # Set image resolution scale (scale=1 corresponds to a standard 72 DPI image)\n",
    ")\n",
    "\n",
    "# Initialize the DocumentConverter with the specified pipeline options\n",
    "doc_converter_global = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# Read the PDF file\n",
    "reader = PdfReader(r\"data\\pdfs\\WEF_The_Global_Cooperation_Barometer_2024.pdf\")\n",
    "writer = PdfWriter()\n",
    "\n",
    "OUTPUT = Path(r\"data\\pdfs\\page_14.pdf\")\n",
    "\n",
    "# Ensure the requested page exists in the document\n",
    "if len(reader.pages) >= 14:\n",
    "    # Add page 14 (index 13 since it's 0-based)\n",
    "    writer.add_page(reader.pages[13])\n",
    "else:\n",
    "    print(\"The PDF does not contain 14 pages.\")\n",
    "\n",
    "# Save the extracted page to a new PDF file\n",
    "with OUTPUT.open('wb') as output_pdf:\n",
    "    writer.write(output_pdf)\n",
    "\n",
    "print(f\"Page 14 has been saved to {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = doc_converter_global.convert(OUTPUT)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT.exists():\n",
    "    OUTPUT.unlink()\n",
    "    print(f\"Arquivo {OUTPUT} excluído com sucesso.\")\n",
    "else:\n",
    "    print(f\"Arquivo {OUTPUT} não existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\html\\python - How to run DeepSeek model locally - Stack Overflow.html\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diversos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\docs\\word.docx\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\docs\\xlsx.xlsx\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytesseract test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "#pytesseract.pytesseract.tesseract_cmd = r\"E:\\programas\\ia\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "image_path = r\"data\\imagens\\Captura de tela 2025-03-13 085540.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "extracted_text = pytesseract.image_to_string(image, lang=LANG)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### direct chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = doc_converter_global.convert(Path(r\"data\\pdfs\\monopoly.pdf\"))\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#conv_res = DocumentConverter().convert(FIRST_10_PAGES)\n",
    "#doc = conv_res.document\n",
    "\n",
    "chunker = HybridChunker(tokenizer=\"BAAI/bge-small-en-v1.5\")  # set tokenizer as needed\n",
    "chunk_iter = chunker.chunk(result.document)\n",
    "\n",
    "# Convert the iterator to a list to count the chunks\n",
    "chunks = list(chunk_iter)\n",
    "num_chunks = len(chunks)\n",
    "\n",
    "# Print the number of chunks\n",
    "print(f\"The document has been divided into {num_chunks} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chroma chunking\n",
    "\n",
    "obs: precisa do ollama executando `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(result.document.export_to_markdown())\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "local_path = r\"data\\pdfs\\monopoly.pdf\"\n",
    "result = doc_converter_global.convert(Path(local_path))\n",
    "documento = Document(page_content=result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED), metadata={\"source\": local_path})\n",
    "\n",
    "print(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents([documento])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separar documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def separar_arquivos(diretorio):\n",
    "    \"\"\"\n",
    "    Varre um diretório e suas subpastas, separando arquivos de imagem de outros tipos de arquivo.\n",
    "\n",
    "    Args:\n",
    "        diretorio (str): O caminho do diretório a ser varrido.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo duas listas: imagens e documentos.\n",
    "    \"\"\"\n",
    "\n",
    "    imagens = []\n",
    "    documentos = []\n",
    "\n",
    "    extensoes_imagens = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']  # Adicione outras extensões se necessário\n",
    "\n",
    "    for raiz, subpastas, arquivos in os.walk(diretorio):\n",
    "        for arquivo in arquivos:\n",
    "            caminho_arquivo = os.path.join(raiz, arquivo)\n",
    "            nome_arquivo, extensao = os.path.splitext(arquivo)\n",
    "            extensao = extensao.lower()\n",
    "\n",
    "            if extensao in extensoes_imagens:\n",
    "                imagens.append(caminho_arquivo) #adiciona o caminho completo\n",
    "            else:\n",
    "                documentos.append(caminho_arquivo) #adiciona o caminho completo\n",
    "\n",
    "    return imagens, documentos\n",
    "\n",
    "imagens, documentos = separar_arquivos(PATH_ARQUIVOS)\n",
    "\n",
    "# print(\"Imagens:\")\n",
    "# for imagem in imagens: print(imagem)\n",
    "\n",
    "# print(\"\\nDocumentos:\")\n",
    "# for documento in documentos: print(documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# display(result.document.export_to_markdown())\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# img\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "#pytesseract.pytesseract.tesseract_cmd = r\"E:\\programas\\ia\\Tesseract-OCR\\tesseract.exe\"\n",
    "#end img\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "\n",
    "#local_path = r\"data\\pdfs\\monopoly.pdf\"\n",
    "def get_chunks_doc(local_path):\n",
    "    result = doc_converter_global.convert(Path(local_path))\n",
    "    documento = Document(page_content=result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED), metadata={\"source\": local_path})\n",
    "    chunks = text_splitter.split_documents([documento])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_chunks_image(local_path):\n",
    "    image = Image.open(local_path)\n",
    "    extracted_text = pytesseract.image_to_string(image, lang=LANG)\n",
    "\n",
    "    documento = Document(page_content=extracted_text, metadata={\"source\": local_path})\n",
    "    chunks = text_splitter.split_documents([documento])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chromadb\n",
    "\n",
    "obs: precisa do ollama executando `ollama serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no batch\n",
    "\n",
    "# def check_and_add_document(collection, document, page_index, embedding_model):\n",
    "#     \"\"\"Verifica se um documento já existe na coleção e o adiciona se não existir.\"\"\"\n",
    "#     document_id = generate_id(document, page_index)\n",
    "\n",
    "#     # Verifica se o ID já existe na coleção\n",
    "#     results = collection.get(ids=[document_id])\n",
    "#     if results['ids'] and document_id in results['ids']:\n",
    "#         print(f\"Documento com ID {document_id} já existe na coleção.\")\n",
    "#         return\n",
    "\n",
    "#     # Adiciona o documento à coleção\n",
    "#     embedding = embedding_model.embed_documents([document.page_content])[0]\n",
    "#     collection.add(documents=[document.page_content], ids=[document_id], embeddings=[embedding], metadatas=[document.metadata])\n",
    "#     print(f\"Documento com ID {document_id} adicionado à coleção.\")\n",
    "\n",
    "# # exemplo inline de chunks\n",
    "# # chunks = [\n",
    "# #     Document(page_content=\"This is a document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "# #     Document(page_content=\"This is a document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "# #     Document(page_content=\"Another document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "# #     Document(page_content=\"New document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "# # ]\n",
    "\n",
    "# embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# # Inicializa o cliente ChromaDB\n",
    "# chroma_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=False))\n",
    "\n",
    "# collection_name = \"local-rag\"\n",
    "\n",
    "# def chroma_indexing(chunks, collection_name = \"local-rag\"):\n",
    "#     # Obtém ou cria a coleção\n",
    "#     collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "#     print(\"Coleção carregada.\")\n",
    "\n",
    "#     # Itera sobre os chunks e os adiciona à coleção, verificando se já existem\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         check_and_add_document(collection, chunk, i, embedding_model)\n",
    "\n",
    "#     # Exemplo para verificar o ID do primeiro chunk\n",
    "#     if chunks:\n",
    "#         first_chunk_id = generate_id(chunks[0], 0)\n",
    "#         print(f\"ID do primeiro chunk: {first_chunk_id}\")\n",
    "\n",
    "#     # # Exemplo de query para verificar se os dados foram adicionados corretamente\n",
    "#     # query_embedding = embedding_model.embed_query(\"fruit information\")\n",
    "#     # query_results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "#     # print(\"\\nResults of query:\")\n",
    "#     # for result in query_results['documents'][0]:\n",
    "#     #     print(result)\n",
    "#     return collection\n",
    "\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chroma batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from pathlib import Path\n",
    "\n",
    "def generate_id_filename(filename, page_index):\n",
    "    filename = os.path.basename(filename)\n",
    "    base_id = hashlib.sha256(filename.encode()).hexdigest()\n",
    "    return f\"{base_id}_{page_index}\"    \n",
    "\n",
    "def generate_id(document, page_index):\n",
    "    \"\"\"Gera um ID único baseado no nome do arquivo e no índice da página.\"\"\"\n",
    "    source = document.metadata['source']\n",
    "    return generate_id_filename(os.path.basename(source), page_index)\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "chroma_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=False))\n",
    "COLLECTION_NAME = \"local-rag\"\n",
    "\n",
    "def chroma_indexing(path_arquivos=PATH_ARQUIVOS, collection_name=COLLECTION_NAME, embedding_model=embedding_model, chroma_client=chroma_client):\n",
    "    \"\"\"Indexa chunks em lote no ChromaDB.\"\"\"\n",
    "\n",
    "    imagens, documentos = separar_arquivos(path_arquivos)\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for imagem in imagens:\n",
    "        id_aux = generate_id_filename(imagem, 0)\n",
    "        results = collection.get(ids=[id_aux])\n",
    "        if results['ids'] and id_aux in results['ids']:\n",
    "            print(f\"Documento com ID {id_aux} | {os.path.basename(imagem)} já existe na coleção.\")\n",
    "            continue\n",
    "        \n",
    "        chroma_indexing_batch(get_chunks_image(imagem), collection, embedding_model)\n",
    "\n",
    "    for documento in documentos:\n",
    "        id_aux = generate_id_filename(documento, 0)\n",
    "        results = collection.get(ids=[id_aux])\n",
    "        if results['ids'] and id_aux in results['ids']: \n",
    "            print(f\"Documento com ID {id_aux} | {os.path.basename(documento)} já existe na coleção.\")\n",
    "            continue\n",
    "\n",
    "        chroma_indexing_batch(get_chunks_doc(documento), collection, embedding_model)\n",
    "\n",
    "    return collection\n",
    "    \n",
    "\n",
    "def chroma_indexing_batch(chunks, collection=None, embedding_model=embedding_model):\n",
    "    \"\"\"Indexa chunks em lote no ChromaDB.\"\"\"\n",
    "\n",
    "    if not chunks or not collection:\n",
    "        print('Sem chunks e/ou collection is null')\n",
    "        return\n",
    "\n",
    "    documents_to_add = []\n",
    "    ids_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        document_id = generate_id(chunk, i)\n",
    "        results = collection.get(ids=[document_id])\n",
    "\n",
    "        if results['ids'] and document_id in results['ids']:\n",
    "            print(f\"Documento com ID {document_id} já existe na coleção.\")\n",
    "            continue\n",
    "\n",
    "        embedding = embedding_model.embed_documents([chunk.page_content])[0]\n",
    "\n",
    "        documents_to_add.append(chunk.page_content)\n",
    "        ids_to_add.append(document_id)\n",
    "        embeddings_to_add.append(embedding)\n",
    "        metadatas_to_add.append(chunk.metadata)\n",
    "\n",
    "    if documents_to_add:\n",
    "        collection.add(documents=documents_to_add, ids=ids_to_add, embeddings=embeddings_to_add, metadatas=metadatas_to_add)\n",
    "        print(f\"Adicionados {len(documents_to_add)} documentos em lote.\")\n",
    "\n",
    "    if chunks:\n",
    "        first_chunk_id = generate_id(chunks[0], 0)\n",
    "        print(f\"ID do primeiro chunk: {first_chunk_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_indexing(PATH_ARQUIVOS, COLLECTION_NAME, embedding_model, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "results = collection.get(limit=5)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagens, documentos = separar_arquivos(PATH_ARQUIVOS)\n",
    "# for documento in documentos:\n",
    "#     id_aux = generate_id_filename(imagem, 0)\n",
    "\n",
    "\n",
    "# collection_name = \"local-rag\"\n",
    "\n",
    "# for imagem in imagens:\n",
    "#     chunks = get_chunks_image(imagem)\n",
    "#     if (chunks is None or len(chunks) <= 0): continue\n",
    "#     chroma_indexing(chunks, collection_name)\n",
    "\n",
    "# for documento in documentos:\n",
    "#     chunks = get_chunks_doc(documento)\n",
    "#     if (chunks is None or len(chunks) <= 0): continue\n",
    "#     chroma_indexing(chunks, collection_name)\n",
    "\n",
    "# print(\"Documentos indexados\")\n",
    "#----------------------------------------------\n",
    "# for imagem in imagens:\n",
    "#     chunks = get_chunks_image(imagem)\n",
    "#     if chunks:\n",
    "#         chroma_indexing_batch(chunks, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "# for documento in documentos:\n",
    "#     chunks = get_chunks_doc(documento)\n",
    "#     if chunks:\n",
    "#         chroma_indexing_batch(chunks, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "# print(\"Documentos indexados\")\n",
    "#----------------------------------------------\n",
    "# chunks_list = []\n",
    "\n",
    "# for imagem in imagens:\n",
    "#     chunks = get_chunks_image(imagem)\n",
    "#     if chunks: chunks_list.append(chunks)\n",
    "\n",
    "# for documento in documentos:\n",
    "#     chunks = get_chunks_doc(documento)\n",
    "#     if chunks: chunks_list.append(chunks)\n",
    "\n",
    "# chroma_indexing_batch(chunks_list, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "# print(\"Documentos indexados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"llama3.2\"\n",
    "local_model = \"deepseek-r1\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Você é um assistente de modelo de linguagem de IA. Sua tarefa é gerar cinco\n",
    "    versões diferentes da pergunta do usuário fornecida para recuperar documentos relevantes de\n",
    "    um banco de dados vetorial. Ao gerar múltiplas perspectivas sobre a pergunta do usuário, seu\n",
    "    objetivo é ajudar o usuário a superar algumas das limitações da pesquisa de similaridade \n",
    "    baseada em distância. Forneça essas perguntas alternativas separadas por quebras de linha.\n",
    "    Pergunta original: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "vector_db = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Responda à pergunta com base SOMENTE no seguinte contexto:\n",
    "{context}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(chain.invoke(\"Como jogar Monopoly ?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(chain.invoke(\"Explique um Voice-Enabled AI e qual a conclusão dos autores ?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(chain.invoke(\"Como executar o DeepSeek com python local ?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excluir Coleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()\n",
    "print(f\"Coleção '{COLLECTION_NAME}' deletada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_3129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

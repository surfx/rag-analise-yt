{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ARQUIVOS = r\"data\"\n",
    "LANG = \"por\" # por eng\n",
    "persist_directory = \"chroma/chroma_db\"  # Diretório onde o banco de dados será salvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode, EasyOcrOptions, TesseractOcrOptions, OcrMacOptions\n",
    "from docling.datamodel.settings import settings\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "# Define pipeline options for PDF processing\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    do_table_structure=True,  # Enable table structure detection\n",
    "    do_ocr=True,  # Enable OCR\n",
    "    # full page ocr and language selection\n",
    "    #ocr_options=EasyOcrOptions(force_full_page_ocr=True, lang=[\"en\"]),  # Use EasyOCR for OCR\n",
    "    ocr_options=TesseractOcrOptions(force_full_page_ocr=True, lang=[LANG]),  # Uncomment to use Tesseract for OCR\n",
    "    #ocr_options = OcrMacOptions(force_full_page_ocr=True, lang=['en-US']),\n",
    "    table_structure_options=dict(\n",
    "        do_cell_matching=False,  # Use text cells predicted from table structure model\n",
    "        mode=TableFormerMode.ACCURATE  # Use more accurate TableFormer model\n",
    "    ),\n",
    "    generate_page_images=True,  # Enable page image generation\n",
    "    generate_picture_images=True,  # Enable picture image generation\n",
    "    images_scale=IMAGE_RESOLUTION_SCALE, # Set image resolution scale (scale=1 corresponds to a standard 72 DPI image)\n",
    ")\n",
    "\n",
    "# Initialize the DocumentConverter with the specified pipeline options\n",
    "doc_converter_global = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# Read the PDF file\n",
    "reader = PdfReader(r\"data\\pdfs\\WEF_The_Global_Cooperation_Barometer_2024.pdf\")\n",
    "writer = PdfWriter()\n",
    "\n",
    "OUTPUT = Path(r\"data\\pdfs\\page_14.pdf\")\n",
    "\n",
    "# Ensure the requested page exists in the document\n",
    "if len(reader.pages) >= 14:\n",
    "    # Add page 14 (index 13 since it's 0-based)\n",
    "    writer.add_page(reader.pages[13])\n",
    "else:\n",
    "    print(\"The PDF does not contain 14 pages.\")\n",
    "\n",
    "# Save the extracted page to a new PDF file\n",
    "with OUTPUT.open('wb') as output_pdf:\n",
    "    writer.write(output_pdf)\n",
    "\n",
    "print(f\"Page 14 has been saved to {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = doc_converter_global.convert(OUTPUT)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT.exists():\n",
    "    OUTPUT.unlink()\n",
    "    print(f\"Arquivo {OUTPUT} excluído com sucesso.\")\n",
    "else:\n",
    "    print(f\"Arquivo {OUTPUT} não existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\html\\python - How to run DeepSeek model locally - Stack Overflow.html\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diversos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\docs\\word.docx\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(r\"data\\docs\\xlsx.xlsx\")\n",
    "\n",
    "result = doc_converter_global.convert(file)\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytesseract test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "#pytesseract.pytesseract.tesseract_cmd = r\"E:\\programas\\ia\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "image_path = r\"data\\imagens\\Captura de tela 2025-03-13 085540.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "extracted_text = pytesseract.image_to_string(image, lang=LANG)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### direct chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = doc_converter_global.convert(Path(r\"data\\pdfs\\monopoly.pdf\"))\n",
    "display(result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#conv_res = DocumentConverter().convert(FIRST_10_PAGES)\n",
    "#doc = conv_res.document\n",
    "\n",
    "chunker = HybridChunker(tokenizer=\"BAAI/bge-small-en-v1.5\")  # set tokenizer as needed\n",
    "chunk_iter = chunker.chunk(result.document)\n",
    "\n",
    "# Convert the iterator to a list to count the chunks\n",
    "chunks = list(chunk_iter)\n",
    "num_chunks = len(chunks)\n",
    "\n",
    "# Print the number of chunks\n",
    "print(f\"The document has been divided into {num_chunks} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chroma chunking\n",
    "\n",
    "obs: precisa do ollama executando `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(result.document.export_to_markdown())\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "local_path = r\"data\\pdfs\\monopoly.pdf\"\n",
    "result = doc_converter_global.convert(Path(local_path))\n",
    "documento = Document(page_content=result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED), metadata={\"source\": local_path})\n",
    "\n",
    "print(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents([documento])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def separar_arquivos(diretorio):\n",
    "    \"\"\"\n",
    "    Varre um diretório e suas subpastas, separando arquivos de imagem de outros tipos de arquivo.\n",
    "\n",
    "    Args:\n",
    "        diretorio (str): O caminho do diretório a ser varrido.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo duas listas: imagens e documentos.\n",
    "    \"\"\"\n",
    "\n",
    "    imagens = []\n",
    "    documentos = []\n",
    "\n",
    "    extensoes_imagens = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']  # Adicione outras extensões se necessário\n",
    "\n",
    "    for raiz, subpastas, arquivos in os.walk(diretorio):\n",
    "        for arquivo in arquivos:\n",
    "            caminho_arquivo = os.path.join(raiz, arquivo)\n",
    "            nome_arquivo, extensao = os.path.splitext(arquivo)\n",
    "            extensao = extensao.lower()\n",
    "\n",
    "            if extensao in extensoes_imagens:\n",
    "                imagens.append(caminho_arquivo) #adiciona o caminho completo\n",
    "            else:\n",
    "                documentos.append(caminho_arquivo) #adiciona o caminho completo\n",
    "\n",
    "    return imagens, documentos\n",
    "\n",
    "diretorio = r'data'\n",
    "imagens, documentos = separar_arquivos(diretorio)\n",
    "\n",
    "# print(\"Imagens:\")\n",
    "# for imagem in imagens: print(imagem)\n",
    "\n",
    "# print(\"\\nDocumentos:\")\n",
    "# for documento in documentos: print(documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chromadb\n",
    "\n",
    "obs: precisa do ollama executando `ollama serve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chroma batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from pathlib import Path\n",
    "\n",
    "def generate_id(document, page_index):\n",
    "    \"\"\"Gera um ID único baseado no nome do arquivo e no índice da página.\"\"\"\n",
    "    source = document.metadata['source']\n",
    "    filename = os.path.basename(source)\n",
    "    base_id = hashlib.sha256(filename.encode()).hexdigest()\n",
    "    return f\"{base_id}_{page_index}\"\n",
    "\n",
    "# no batch\n",
    "\n",
    "# def check_and_add_document(collection, document, page_index, embedding_model):\n",
    "#     \"\"\"Verifica se um documento já existe na coleção e o adiciona se não existir.\"\"\"\n",
    "#     document_id = generate_id(document, page_index)\n",
    "\n",
    "#     # Verifica se o ID já existe na coleção\n",
    "#     results = collection.get(ids=[document_id])\n",
    "#     if results['ids'] and document_id in results['ids']:\n",
    "#         print(f\"Documento com ID {document_id} já existe na coleção.\")\n",
    "#         return\n",
    "\n",
    "#     # Adiciona o documento à coleção\n",
    "#     embedding = embedding_model.embed_documents([document.page_content])[0]\n",
    "#     collection.add(documents=[document.page_content], ids=[document_id], embeddings=[embedding], metadatas=[document.metadata])\n",
    "#     print(f\"Documento com ID {document_id} adicionado à coleção.\")\n",
    "\n",
    "# # exemplo inline de chunks\n",
    "# # chunks = [\n",
    "# #     Document(page_content=\"This is a document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "# #     Document(page_content=\"This is a document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "# #     Document(page_content=\"Another document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "# #     Document(page_content=\"New document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "# # ]\n",
    "\n",
    "# embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# # Inicializa o cliente ChromaDB\n",
    "# chroma_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=False))\n",
    "\n",
    "# collection_name = \"local-rag\"\n",
    "\n",
    "# def chroma_indexing(chunks, collection_name = \"local-rag\"):\n",
    "#     # Obtém ou cria a coleção\n",
    "#     collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "#     print(\"Coleção carregada.\")\n",
    "\n",
    "#     # Itera sobre os chunks e os adiciona à coleção, verificando se já existem\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         check_and_add_document(collection, chunk, i, embedding_model)\n",
    "\n",
    "#     # Exemplo para verificar o ID do primeiro chunk\n",
    "#     if chunks:\n",
    "#         first_chunk_id = generate_id(chunks[0], 0)\n",
    "#         print(f\"ID do primeiro chunk: {first_chunk_id}\")\n",
    "\n",
    "#     # # Exemplo de query para verificar se os dados foram adicionados corretamente\n",
    "#     # query_embedding = embedding_model.embed_query(\"fruit information\")\n",
    "#     # query_results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "#     # print(\"\\nResults of query:\")\n",
    "#     # for result in query_results['documents'][0]:\n",
    "#     #     print(result)\n",
    "#     return collection\n",
    "\n",
    "# batch\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "chroma_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=False))\n",
    "collection_name = \"local-rag\"\n",
    "\n",
    "def chroma_indexing_batch(chunks, collection_name=\"local-rag\", embedding_model=None, chroma_client=None):\n",
    "    \"\"\"Indexa chunks em lote no ChromaDB.\"\"\"\n",
    "    if not chunks:\n",
    "        return\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    print(\"Coleção carregada.\")\n",
    "\n",
    "    documents_to_add = []\n",
    "    ids_to_add = []\n",
    "    embeddings_to_add = []\n",
    "    metadatas_to_add = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        document_id = generate_id(chunk, i)\n",
    "        results = collection.get(ids=[document_id])\n",
    "\n",
    "        if results['ids'] and document_id in results['ids']:\n",
    "            print(f\"Documento com ID {document_id} já existe na coleção.\")\n",
    "            continue\n",
    "\n",
    "        embedding = embedding_model.embed_documents([chunk.page_content])[0]\n",
    "\n",
    "        documents_to_add.append(chunk.page_content)\n",
    "        ids_to_add.append(document_id)\n",
    "        embeddings_to_add.append(embedding)\n",
    "        metadatas_to_add.append(chunk.metadata)\n",
    "\n",
    "    if documents_to_add:\n",
    "        collection.add(documents=documents_to_add, ids=ids_to_add, embeddings=embeddings_to_add, metadatas=metadatas_to_add)\n",
    "        print(f\"Adicionados {len(documents_to_add)} documentos em lote.\")\n",
    "\n",
    "    if chunks:\n",
    "        first_chunk_id = generate_id(chunks[0], 0)\n",
    "        print(f\"ID do primeiro chunk: {first_chunk_id}\")\n",
    "\n",
    "    return collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# display(result.document.export_to_markdown())\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# img\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "#pytesseract.pytesseract.tesseract_cmd = r\"E:\\programas\\ia\\Tesseract-OCR\\tesseract.exe\"\n",
    "#end img\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "\n",
    "#local_path = r\"data\\pdfs\\monopoly.pdf\"\n",
    "def get_chunks_doc(local_path):\n",
    "    result = doc_converter_global.convert(Path(local_path))\n",
    "    documento = Document(page_content=result.document.export_to_markdown(image_mode=ImageRefMode.EMBEDDED), metadata={\"source\": local_path})\n",
    "    chunks = text_splitter.split_documents([documento])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_chunks_image(local_path):\n",
    "    image = Image.open(local_path)\n",
    "    extracted_text = pytesseract.image_to_string(image, lang=LANG)\n",
    "\n",
    "    documento = Document(page_content=extracted_text, metadata={\"source\": local_path})\n",
    "    chunks = text_splitter.split_documents([documento])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção carregada.\n",
      "Adicionados 348 documentos em lote.\n",
      "ID do primeiro chunk: 7c0100cbcab341da2d7e3063ea01534e24cd669c505f7845f3ccc8bd95ddba50_0\n",
      "Coleção carregada.\n",
      "Adicionados 82 documentos em lote.\n",
      "ID do primeiro chunk: 35ee8ca3231e1a9d9ea39b17b42b8c0fc604929073ef313ce42096db30ea7847_0\n",
      "Coleção carregada.\n",
      "Adicionados 266 documentos em lote.\n",
      "ID do primeiro chunk: 6067fc7624ab65a60d140267e037bb518f47f035d79cba85a3bfd421e6007c3a_0\n",
      "Coleção carregada.\n",
      "Adicionados 10 documentos em lote.\n",
      "ID do primeiro chunk: a8f083315429e2f971a0292de1197fef3a7059aa7ef6a1698b1c1d2c0775341e_0\n",
      "Coleção carregada.\n",
      "Adicionados 760 documentos em lote.\n",
      "ID do primeiro chunk: 27c24834bbaf23c523d8d555b4026d3e51710cc16d86a1d1eb55092a793fcbaa_0\n",
      "Coleção carregada.\n",
      "Adicionados 1401 documentos em lote.\n",
      "ID do primeiro chunk: 1cb071d312447aff4693e8b3dffd19573f21df379dfdb79defcca0fd40b25792_0\n",
      "Documentos indexados\n"
     ]
    }
   ],
   "source": [
    "# collection_name = \"local-rag\"\n",
    "\n",
    "# for imagem in imagens:\n",
    "#     chunks = get_chunks_image(imagem)\n",
    "#     if (chunks is None or len(chunks) <= 0): continue\n",
    "#     chroma_indexing(chunks, collection_name)\n",
    "\n",
    "# for documento in documentos:\n",
    "#     chunks = get_chunks_doc(documento)\n",
    "#     if (chunks is None or len(chunks) <= 0): continue\n",
    "#     chroma_indexing(chunks, collection_name)\n",
    "\n",
    "# print(\"Documentos indexados\")\n",
    "\n",
    "for imagem in imagens:\n",
    "    chunks = get_chunks_image(imagem)\n",
    "    if chunks:\n",
    "        chroma_indexing_batch(chunks, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "for documento in documentos:\n",
    "    chunks = get_chunks_doc(documento)\n",
    "    if chunks:\n",
    "        chroma_indexing_batch(chunks, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "print(\"Documentos indexados\")\n",
    "\n",
    "# chunks_list = []\n",
    "\n",
    "# for imagem in imagens:\n",
    "#     chunks = get_chunks_image(imagem)\n",
    "#     if chunks: chunks_list.append(chunks)\n",
    "\n",
    "# for documento in documentos:\n",
    "#     chunks = get_chunks_doc(documento)\n",
    "#     if chunks: chunks_list.append(chunks)\n",
    "\n",
    "# chroma_indexing_batch(chunks_list, collection_name, embedding_model, chroma_client)\n",
    "\n",
    "# print(\"Documentos indexados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['ff355d2b2b6da4cf0f4fff0d3a48f8d7bf920e8e9fd5854d743d94b8d74431cb_0',\n",
       "  '4de61c5de3846ef479128ce449eca7c0e6bacbb02259525bd3120ed54b454ead_0',\n",
       "  '4de61c5de3846ef479128ce449eca7c0e6bacbb02259525bd3120ed54b454ead_1',\n",
       "  '4de61c5de3846ef479128ce449eca7c0e6bacbb02259525bd3120ed54b454ead_2',\n",
       "  '4de61c5de3846ef479128ce449eca7c0e6bacbb02259525bd3120ed54b454ead_3'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['4)\\n\\ndocli\\n\\nimpo:\\nfrom\\nfrom\\nfrom\\nfrom\\nfrom\\n\\nng\\n\\nrt time\\n\\ndocling.datamodel.base models import InputFormat\\n\\ndocling core.types.doc import ImageRefMode\\n\\ndocling.document converter import DocumentConverter, PdfFormatOption\\n\\ndocling.datamodel .pipeline options import PdfPipelineOptions, TableFormerMode, EasyOcrOptions, TesseractOcrOptions, OcrMacOptions\\n\\ndocling.datamodel .settings import settings\\n\\nIMAGE. RESOLUTION SCALE = 2.0\\n\\n* Define pipeline options for PDF processing\\n\\npipe\\n\\n)\\n\\n4 In\\ndoc :\\n\\nQ 92s\\n\\n\\'line options = PdfPipelineOptions(\\n\\ndo table structure=True, /é Enable table structure detection\\ndo ocr=True, %é Enable OCR\\n\\nÉ full page ocr and language selection\\n\\nHocr options=EasyOcrOptions(force full page ocr=True, lang=[\"en\"]), d* Use EasyOCR for OCR\\n\\nocr options=TesseractOcrOptions(force full page ocr=True, lang=[LANG]), %&\\ntHocr options = OcrMacOptions(force full page ocr=True, lang=[\\'en-US\\']),\\ntable structure options=dict(\\n\\ndo cell matching=False, % Use text cells predicted from table structure model\\n\\nmode=TableFormerMode.ACCURATE % Use more accurate TableFormer model\\n»\\ngenerate page images=True, /é Enable page image generation\\ngenerate picture images=True, é Enable picture image generation\\nimages scale=IMAGE RESOLUTION SCALE, é Set image resolution scale (scale=1\\n\\nitialize the DocumentConverter with the specified pipeline options\\nconverter global = DocumentConverter(\\nformat options=f\\n\\nInputFormat.PDF: PdfFormatOption(pipeline options=pipeline options)\\n)\\n\\nIncomment to use Tesseract for OCR\\n\\njbrresponds to a standard 72 DPI image)\\n\\nPython',\n",
       "  '## The quick brown fox jumps over the lazy dog\\n\\n23 línguas\\n\\n- Artigo\\n- Discussão\\n- Ler\\n- Editar\\n- Ver histórico\\n\\nFerramentas\\n\\nAspeto ocultar\\n\\nTexto\\n\\n- Pequeno\\n- Padrão\\n- Grande\\n\\nLargura\\n\\n- Padrão\\n- Largo\\n\\nCor (beta)',\n",
       "  '- Automático\\n- Claro\\n- Noite',\n",
       "  '-',\n",
       "  '![Image](data:image/png;base64,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'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'data\\\\imagens\\\\Captura de tela 2025-03-13 085540.png'},\n",
       "  {'source': 'data\\\\docs\\\\word.docx'},\n",
       "  {'source': 'data\\\\docs\\\\word.docx'},\n",
       "  {'source': 'data\\\\docs\\\\word.docx'},\n",
       "  {'source': 'data\\\\docs\\\\word.docx'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "results = collection.get(limit=5)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"llama3.2\"\n",
    "local_model = \"deepseek-r1\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Você é um assistente de modelo de linguagem de IA. Sua tarefa é gerar cinco\n",
    "    versões diferentes da pergunta do usuário fornecida para recuperar documentos relevantes de\n",
    "    um banco de dados vetorial. Ao gerar múltiplas perspectivas sobre a pergunta do usuário, seu\n",
    "    objetivo é ajudar o usuário a superar algumas das limitações da pesquisa de similaridade \n",
    "    baseada em distância. Forneça essas perguntas alternativas separadas por quebras de linha.\n",
    "    Pergunta original: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "vector_db = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Responda à pergunta com base SOMENTE no seguinte contexto:\n",
    "{context}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Primeiro, lembro que o jogo Monopoly é um jogo de tabuleiro clássico em que os jogadores moveem their tokens nas casas do tabuleiro, obeyindo as regras e tentando ganhar dinheiro comprando e vendendo propriedades. O objetivo geral é acumular mais moedas do que os outros jogadores.\n",
       "\n",
       "Para entender como jogar Monopoly, preciso familiarizar-se com o componente principal: a fundamentals de ruleset, que contém todas as regras do jogo. Isso inclui como se movimenta, como se manipula a moedas, como funciona o brushing (colisões) e os pagamentos, como a imóvel de paradoxo e as special edições, como os jardins e as ruízas.\n",
       "\n",
       "Outro passo é aprender sobre o começo do jogo. Geralmente, os jogadores sortearão um space e escolherão um ibile para commencer. O jogador que der届 à esquerda da mesa vai ser o \"Primeiro Jogador\", e os demais seguem em rota. Em seguida, cada jogador escolhe uma propriedade e deve pagar a entrada correspondente.\n",
       "\n",
       "Depois de cada jogador ter escolhido sua propriedade inicial, o jogo passa para a etapa de ferias. Neste momento, cada jogador pode optar por manter suas propriedades ou vender-las, tentando ganhar dinheiro. Se um jogador der à esquerda da mesa, ele ganha a \"free rent\" (gratuita), que geralmente significa que ele pode的选择 uma propriedade livre.\n",
       "\n",
       "Aqui está onde a complexidade do jogo entra em brilho. Cada propriedade tem um preço inicial, e muitas delas também incluem specialidades, como quartos de bilionária ou parques specially lang, que podem oferecer mais valor ou vantagens ao jogador que as compre.\n",
       "\n",
       "Outra parte crucial é o brushing, ou colisões. Se um jogador está andando e uma casinha ou specialidade aleatória é sortida, ele deve \"bruck\" para ela. Depende do tipo de casinha qual a regra varia. Por exemplo, moveda aleatória pode fazer o jogador parar, enquanto a imóvel de paradoxo põe o jogador em火烧.\n",
       "\n",
       "As special edições, como os de jardins ou ruízas, adicionam mais levels de interação e ganhos ao jogo. por exemplo, um jardim de rose ou uma rua de negativo podem oferecer oportunidades adicionais de ganho ou perda de dinheiro.\n",
       "\n",
       "Uma parte importante também é o estoque de moedas. Jogadores usam as moedas sorteadas para pagar as despesas e manter suas propriedades em andamento. Se um jogador não tiver suficiente moedas, pode ser atraído para a simplesidade de vender uma propriedade para ganhar dinheiro.\n",
       "\n",
       "Ainda, o jogo inclui specialidades como os esquema de jogos, que permitem que jogadores \"bruck\" para espaço se as specialidades forem sortidas. Talvez tenhamos um esquema de jogos de jardins ou imóveis de paradoxo que possam oferecer mais beneficios.\n",
       "\n",
       "Adiante está a etapa de vencedor. O objetivo é ganhar dinheiro, mas em Monopoly, há também o Prêmio do Global, que é premiado a quem tem o maior patrimônio no final de um período determinado. Além disso, há a possibilidade de um jogador ser \"empréstimo\" ou \"devedor\" de uma propriedade, influenciando o fluxo de dinheiro entre os jogadores.\n",
       "\n",
       "Em resumo, para aprender a jogar Monopoly, é essencial entender as regras básicas, a etapa de começo, como lidar com brushing e specialidades, e como manipular o estoque de moedas para manter um equilíbrio entre ganho de dinheiro e manutenção de propriedades. Com time spent practicando e experimentando diferentes战略s, um jogador pode melhorar suas habilidades e aumentar as chances de vencer.\n",
       "</think>\n",
       "\n",
       "**Começando a jogar Monopoly:**\n",
       "\n",
       "1. **Regras do Jogo:**\n",
       "   - Revise o fundamentals de ruleset para familiarizar-se com todas as regras do jogo, incluindo movimentos, manutenção de propriedades, brushing (colisões), pagamentos e specialidades como a imóvel de paradoxo.\n",
       "\n",
       "2. **Começo do Jogo:**\n",
       "   - Sorteie um espaço inicial para determinar o primeiro jogador.\n",
       "   - Cada jogador escolhe uma propriedade initial, pagando a entrada correspondente.\n",
       "\n",
       "3. **Férias e Esquema de Jogos:**\n",
       "   - Durante as férias, cada jogador pode optar por manter suas propriedades ou vender-las.\n",
       "   - Se um jogador está à esquerda da mesa, recebe a \"free rent\" (gratuita) e pode escolher uma propriedade livre.\n",
       "\n",
       "4. ** lidar com Brushing:**\n",
       "   - Caso uma casinha ou specialidade aleatória seja sortida, o jogador deve \"bruck\" para ela.\n",
       "   - Cada tipo de casinha tem regras específicas; por exemplo, a imóvel de paradoxo põe o jogador em火烧.\n",
       "\n",
       "5. **Specialidades:**\n",
       "   - As specialidades como jardins e ruízas adicioneam mais levels de interação e possuem influências adicionais no jogo.\n",
       "   - Alguns esquemas de jogos permitem que jogadores \"bruck\" para espaço se as specialidades forem sortidas.\n",
       "\n",
       "6. **Ganhando Dinheiro:**\n",
       "   - Jogadores usam moedas sorteadas para pagar despesas e manter suas propriedades em andamento.\n",
       "   - Possíveis premios incluem o Prêmio do Global e o role de \"empréstimo\" ou \"devedor\".\n",
       "\n",
       "7. **Etapa de Vencedor:**\n",
       "   - O objetivo é ganhar dinheiro, mas o jogo também inclui premios baseados no patrimônio acumulado.\n",
       "   - Se um jogador tiver muito dinheiro em contraposição aos demais jogadores, pode ser \"empréstimo\" ou \"devedor\".\n",
       "\n",
       "**Conclusão:**\n",
       "Para aprender a jogar Monopoly, é essencial compreender as regras básicas, a etapa de começo, como lidar com brushing e specialidades, e como manipular o estoque de moedas. Com time spent practicando e experimentando diferentes estratégias, um jogador pode melhorar suas habilidades e aumentar as chances de vencer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(chain.invoke(\"Como jogar Monopoly ?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Primeiro, preciso entender o que é um Voice-Enabled AI. Parece que ele é um tipo de inteligência artificial que está associado ao voz, ou seja, pode ser controlado com as palavras de um usuário. Isso inclui habilidades como falar respostas, assistir a videoaulas e realizar tarefas específicas.\n",
       "\n",
       "Agora, leio o texto que o usuário forneceu. Ele parece estar falando sobre uma avaliação ou uma análise de um método para usar AI em data science, comumente chamado de DS-1000. Na seção onde ele menciona \"propRIETARIOS\" e \"abordagens\", estou acreditando que ele esteja falando sobre um estudo que comparou diferentes tipos de modelo de AI propRIETARIOS contra os Aberto-sourse.\n",
       "\n",
       "Então, a pergunta é: \"Explain um Voice-Enabled AI and what is the conclusion of the authors?\" Isso sugere que ele quer a minha explicação sobre o que é o Voice-Enabled AI e quais as conclusões drawn dos autores do texto fornecido.\n",
       "\n",
       "No entanto, o texto fornecido não menciona diretamente nothing sobre Voice-Enabled AI. Parece que ele esteja mais focado na avaliação de modelos de AI propRIETARIOS e suas abordagens em data science, como usar Chain-of-Thought Research ou Instruções de Passagem Rápida (Five-shot prompting) com exemplos relevantes.\n",
       "\n",
       "Pois, pode-se inferir que a conclusão dos autores é sobre a performance dos modelos propRIETARIOS em tarefas específicas de data science, especialmente em comparação com os Aberto-sourse. Eles mentionam que os models propRIETARIOS performaram melhor, especialmente nos benchmarks como DS-1000, onde models propRIETARIOS superaram significativamente os Aberto-sourse.\n",
       "\n",
       "Portanto, a conclusão dos autores, no contexto do texto fornecido, parece ser que os models propRIETARIOS são mais indicados para tarefas de data science devido à melhor performance em benchmarks relevantes, e que usar abordagens como Chain-of-Thought Research pode ajudar a melhorar ainda mais o desempenho dos models.\n",
       "\n",
       "Finalmente, para responder à pergunta, preciso congregar essas informações: explica brevemente o Voice-Enabled AI e menciona as conclusões dos autores sobre os modelos propRIETARIOS em data science.\n",
       "</think>\n",
       "\n",
       "Um **Voice-Enabled AI** (IntelliAGente com habilidades de voz) é uma inteligência artificial que pode ser controlada e interagido com através de palavras. Este tipo de AI pode executar tarefas como falar respostas, processar speech-to-text, assistir a videoaulas e realizar diversas operações específicas, tornando-o útil em contextos como atendimento ao cliente, instruções de trabalho ou interação educationais.\n",
       "\n",
       "A **conclusão dos autores** dos estudos mencionados parece ser que os modelos propRIETARIOS ( proprietary AI models) demonstraram uma performance superior em tarefas de data science, especialmente em benchmarks como DS-1000. Em particular, os models propRIETARIOS superaram significativamente os models Aberto-sourse ( open-source models), com destaque para a capacidade de Chain-of-Thought Research e abordagens como Instruções de Passagem Rápida (Five-shot prompting) com exemplos relevantes em melhorar o desempenho dos models. Assim, os autores sugerem que os models propRIETARIOS são mais indicados para tarefas de data science devido à melhor performance em benchmarks relevantes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chain.invoke(\"Explique um Voice-Enabled AI e qual a conclusão dos autores ?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Para executar o DeepSeek com Python local, siga os seguintes passos:\n",
       "\n",
       "1. **Baixar e Configurar o Framework**:\n",
       "   - Download o framework DeepSeek do Repositório oficial.\n",
       "   - Conecte ao Git para atualizar as dependentes.\n",
       "\n",
       "2. ** instalar As Dependências**:\n",
       "   - Execute `git install .` no terminal para baixar e instalar todas as dependências necessárias.\n",
       "\n",
       "3. **Configurar a Pasta de Trabalho**:\n",
       "   - Definir a pasta raiz onde estão armazenados os arquivos do projeto.\n",
       "\n",
       "4. **Treinar o Modelo**:\n",
       "   - Utilize o script `train.py` para treinar o modelo DeepSeek com as configurações desejadas.\n",
       "\n",
       "5. **Carregar e Executar o Modelo**:\n",
       "   - Carregue o modelo treinado em um ambiente Python local.\n",
       "   - Execute as instruções específicas para interagir com o modelo DeepSeek.\n",
       "\n",
       "6. **Testar em Produzir Texto**:\n",
       "   - Teste diferentes tarefas de geração de texto para avaliar a performance do modelo.\n",
       "\n",
       "7. **Optimizar e Ajustar Parâmetros**:\n",
       "   - Realize ajustes necessários nas configurações para otimização do desempenho.\n",
       "- **Documentação e Suporte**:\n",
       "   - Consulte as documentações disponíveis para resolver qualquer dúvida ou problema em torno do uso do DeepSeek.\n",
       "\n",
       "By following these steps, you can successfully run DeepSeek in your local Python environment and start utilizing its powerful text generation capabilities.\n",
       "</think>\n",
       "\n",
       "Para executar o DeepSeek com Python local, siga os seguintes passos:\n",
       "\n",
       "### 1. Baixar e Configurar o Framework\n",
       "Primeiramente, você precisará obter o Repositório do DeepSeek no GitHub para que possa configurá-lo corretamente.\n",
       "\n",
       "```bash\n",
       "git clone https://github.com/yourusername/deepseek.git\n",
       "cd deepseek\n",
       "```\n",
       "\n",
       "### 2. Instalar as Dependências\n",
       "Antes deanything, atualize as dependências do framework:\n",
       "\n",
       "```bash\n",
       "git install .\n",
       "```\n",
       "\n",
       "### 3. Configurar a Pasta de Trabalho\n",
       "Definir a pasta raiz onde estão armazenados os arquivos do projeto:\n",
       "\n",
       "```python\n",
       "BASEPATH = '/path/to/your/project'\n",
       "```\n",
       "\n",
       "### 4. Treinar o Modelo\n",
       "Treine o modelo usando o script de treinamento fornecido:\n",
       "\n",
       "```bash\n",
       "python train.py --config config/training.cfg\n",
       "```\n",
       "\n",
       "### 5. Carregar e Executar o Modelo\n",
       "Carregue o modelo treinado e execute as instruções desejadas para interagir com o DeepSeek.\n",
       "\n",
       "```python\n",
       "from deepseek import DeepSeek\n",
       "\n",
       "# Caminho para o arquivo do modelo treinado\n",
       "model_path = 'path/to/trained_model'\n",
       "\n",
       "# Inicialize o cliente\n",
       "client = DeepSeek(\n",
       "    model_dir=model_path,\n",
       "    base_path=BASEPATH,\n",
       ")\n",
       "\n",
       "# Execute uma instrução\n",
       "response = client.run(\"Resolva: Digite um texto em português e eu responderei com um texto em inglês, \"\n",
       "                      \"usando as minas para identificar excessos de conteúdo ou excessos de vocabulário.\")\n",
       "print(response)\n",
       "```\n",
       "\n",
       "### 6. Testar em ProduzIR Texto\n",
       "Geralmente, após o treinamento, você pode testar a capacidade de geração de texto com instruções específicas.\n",
       "\n",
       "```python\n",
       "# Gera um texto com base em uma instrução\n",
       "prompt = \"Dado que a cidade tem 10.000 habitantes e 5 estabelecimentos de saneamento, \"\n",
       "         \"qual é a expectativa de população por km²?\"\n",
       " \n",
       "response = client.generate_text(\n",
       "    prompt=prompt,\n",
       "    max_tokens=100,\n",
       ")\n",
       "print(\"Resposta:\", response)\n",
       "```\n",
       "\n",
       "### 7. Optimize e Ajustar Parâmetros\n",
       "Seu modelo pode precisar de ajustes específicos, como alterar hiperparâmetros de treinamento ou otimizar para uma determinada tarefa.\n",
       "\n",
       "```python\n",
       "# Exemplo de ajuste noprompto\n",
       "response = client.run(f\"Treine um modelo com learning rate 0.001 e batch size de 32.\")\n",
       "```\n",
       "\n",
       "### Documentação e Suporte\n",
       "Para mais informações sobre as funcionalidades do DeepSeek, consulte a documentação disponível na seção de documento do Repositório GitHub.\n",
       "\n",
       "```bash\n",
       "cd deepseek/docs\n",
       "```\n",
       "\n",
       "### Conclusão\n",
       "Com estas passos, você está prontinho para usar o DeepSeek no seu local em Python. Basta executar os comandos e ajustar conforme necessário suas instruções."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chain.invoke(\"Como executar o DeepSeek com python local ?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excluir Coleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()\n",
    "print(f\"Coleção '{collection_name}' deletada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_3129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

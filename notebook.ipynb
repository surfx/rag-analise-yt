{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\t\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo com UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from IPython.display import display as Markdown\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = r\"data\\WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first page\n",
    "print(Markdown(data[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo com docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "local_path = r\"data\\WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "#artifacts_path = \"C:/Users/emerson/.cache/docling/models\"\n",
    "# pipeline_options = PdfPipelineOptions(artifacts_path=artifacts_path)\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "result = converter.convert(local_path)\n",
    "print(result.document.export_to_markdown())  # output: \"### Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## olama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pull nomic-embed-text model from Ollama if you don't have it\n",
    "# !ollama pull nomic-embed-text\n",
    "# # List models again to confirm it's available\n",
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. First clean up any existing ChromaDB installations\n",
    "# %pip uninstall -y chromadb\n",
    "# %pip uninstall -y protobuf\n",
    "\n",
    "# # 2. Install specific versions known to work together\n",
    "# %pip install -q protobuf==3.20.3\n",
    "# %pip install -q chromadb==0.4.22  # Using a stable older version\n",
    "# %pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set the environment variable\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import EasyOcrOptions, PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "local_path = r\"data\\WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "#artifacts_path = \"C:/Users/emerson/.cache/docling/models\"\n",
    "# pipeline_options = PdfPipelineOptions(artifacts_path=artifacts_path)\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "result = converter.convert(local_path)\n",
    "print(result.document.export_to_markdown())  # output: \"### Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(result.document.export_to_markdown())\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documento = Document(page_content=result.document.export_to_markdown(), metadata={\"source\": local_path})\n",
    "\n",
    "print(documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents([documento])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: precisa exceutar o ollama aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexar Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "\n",
    "def generate_id(document, page_index):\n",
    "    \"\"\"Gera um ID único baseado no nome do arquivo e no índice da página.\"\"\"\n",
    "    source = document.metadata['source']\n",
    "    filename = os.path.basename(source)\n",
    "    base_id = hashlib.sha256(filename.encode()).hexdigest()\n",
    "    return f\"{base_id}_{page_index}\"\n",
    "\n",
    "def check_and_add_document(collection, document, page_index, embedding_model):\n",
    "    \"\"\"Verifica se um documento já existe na coleção e o adiciona se não existir.\"\"\"\n",
    "    document_id = generate_id(document, page_index)\n",
    "\n",
    "    # Verifica se o ID já existe na coleção\n",
    "    results = collection.get(ids=[document_id])\n",
    "    if results['ids'] and document_id in results['ids']:\n",
    "        print(f\"Documento com ID {document_id} já existe na coleção.\")\n",
    "        return\n",
    "\n",
    "    # Adiciona o documento à coleção\n",
    "    embedding = embedding_model.embed_documents([document.page_content])[0]\n",
    "    collection.add(documents=[document.page_content], ids=[document_id], embeddings=[embedding], metadatas=[document.metadata])\n",
    "    print(f\"Documento com ID {document_id} adicionado à coleção.\")\n",
    "\n",
    "# exemplo inline de chunks\n",
    "# chunks = [\n",
    "#     Document(page_content=\"This is a document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "#     Document(page_content=\"This is a document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "#     Document(page_content=\"Another document about pineapple\", metadata={\"source\": \"file1.txt\"}),\n",
    "#     Document(page_content=\"New document about oranges\", metadata={\"source\": \"file2.txt\"}),\n",
    "# ]\n",
    "\n",
    "collection_name = \"local-rag\"\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "persist_directory = \"chroma/chroma_db\"  # Diretório onde o banco de dados será salvo\n",
    "\n",
    "# Inicializa o cliente ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=False))\n",
    "\n",
    "# Obtém ou cria a coleção\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "print(\"Coleção carregada.\")\n",
    "\n",
    "# Itera sobre os chunks e os adiciona à coleção, verificando se já existem\n",
    "for i, chunk in enumerate(chunks):\n",
    "    check_and_add_document(collection, chunk, i, embedding_model)\n",
    "\n",
    "# Exemplo para verificar o ID do primeiro chunk\n",
    "if chunks:\n",
    "    first_chunk_id = generate_id(chunks[0], 0)\n",
    "    print(f\"ID do primeiro chunk: {first_chunk_id}\")\n",
    "\n",
    "# # Exemplo de query para verificar se os dados foram adicionados corretamente\n",
    "# query_embedding = embedding_model.embed_query(\"fruit information\")\n",
    "# query_results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "# print(\"\\nResults of query:\")\n",
    "# for result in query_results['documents'][0]:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTE Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "chroma_client = chromadb.PersistentClient(path=r\"chroma\\testes\", settings=Settings(allow_reset=True))\n",
    "chroma_client.reset()\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"teste-rag\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about hawaii\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.get(limit=5)\n",
    "display(results)\n",
    "\n",
    "# # Os resultados são um dicionário; você precisa processá-los para obter os documentos\n",
    "# ids = results['ids']\n",
    "# documents = results['documents']\n",
    "# metadatas = results['metadatas']\n",
    "\n",
    "# for i in range(len(ids)):\n",
    "#     print(f\"ID: {ids[i]}\")\n",
    "#     print(f\"Conteúdo: {documents[i]}\")\n",
    "#     print(f\"Metadados: {metadatas[i]}\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"llama3.2\"\n",
    "local_model = \"deepseek-r1\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Você é um assistente de modelo de linguagem de IA. Sua tarefa é gerar cinco\n",
    "    versões diferentes da pergunta do usuário fornecida para recuperar documentos relevantes de\n",
    "    um banco de dados vetorial. Ao gerar múltiplas perspectivas sobre a pergunta do usuário, seu\n",
    "    objetivo é ajudar o usuário a superar algumas das limitações da pesquisa de similaridade \n",
    "    baseada em distância. Forneça essas perguntas alternativas separadas por quebras de linha.\n",
    "    Pergunta original: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "#vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "vector_db = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Responda à pergunta com base SOMENTE no seguinte contexto:\n",
    "{context}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"Quais são os 5 pilares da cooperação global?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excluir Coleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()\n",
    "print(f\"Coleção '{collection_name}' deletada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_3129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
